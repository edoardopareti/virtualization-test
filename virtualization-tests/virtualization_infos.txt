HOST: 
TESTED ON UBUNTU 22.04 LTS
The following was tested using the following CPUs for the host system:
an AMD Ryzen 5 1500X (first) and a Ryzen 5 5600G (then) 
The host system also mounts a NVIDIA GeForce GTX 1060 3GB

CHECK IF HARDWARE ASSISTED VIRTUALIZATION IS ENABLED
egrep -c '(vmx|svm)' /proc/cpuinfo
CHECK IF KVM KERNEL MODULE IS READY
sudo apt install cpu-checker -y
kvm-ok
INSTALL REQUIRED DEPENDANCIES:
QEMU IS THE EMULATOR USING THE KVM ACCELERATOR, LIBVIRT ARE THE BACKEND APIs, AND VIRT-MANAGER THE GRAPHICAL TOOL TO MANAGE VMs
sudo apt install qemu-kvm libvirt-daemon-system virt-manager -y
CHECK IF APIS ARE RUNNING
sudo systemctl status libvirtd
START VIRT-MANAGER
virt-manager

GUEST:

LINUX (x86):
The process of creating a Linux-based guest should not require to access advanced options, at least with an Ubuntu22.04 LTS distribution
(everything should work with the 5 configuration default steps:
(choose the installation method, choose the ISO, set up RAM and CPU, setup VM storage, set the VM name) from the GUI).
You can however read also what follows to better configure your system.

WINDOWS 11
From virt-manager (The KVM/QEMU GUI):
In preferences: Enable XML editing
Follow the 5 configuration steps (common for Linux install too), but before starting the VM
enter the VM Details (Enable "Customize configuration before install") where you can set the following:
Overview - Chipset (Virtualized MOBO chipset. also called machine type) : Q35 (more modern virtualized mobo chipset)
Overview - Firmware: UEFI (UEFI is required for booting Windows 11)
CPU Configuration: 
IF YOU WANT TO EMULATE THE CPU (like a type2 hypervisor)
Disable "Copy host CPU configuration (host-passthrough)" 
(On some CPUs and OS like Windows, host-passthrough will result in an error at boot, so mimicking the host's CPU configuration 
(model/family, features like the ISA, clock speed, cores and threads, cache size, security features, and capabilities 
like hardware-assisted virtualization support for the CPU and I/O operations) as closely as possible !) may yield an error. 
So you can instead select a suitable CPU model - EPYC (for AMD cpus).
BUT YOU CAN ALSO PASSTHROUGH THE CPU (RECOMMENDED) ON WINDOWS 11 WHILE AVOIDING THE WINDOWS BOOT ERROR ABOVE:
Enable "Copy host CPU configuration (host-passthrough)"
sudo -i
echo "options kvm ignore_msrs=1" > /etc/modprobe.d/kvm.conf
This allows direct CPU passthrough to Windows hosts when using an AMD processor, making effective use of the processor
virtualization support and making the QEMU/KVM system very close to a bare metal type 1 hypervisor!
Then select "Manually Set CPU Topology", mimicking cores and threads of your CPU (socket will be 1 as probably you have a single processor)
Download VirtIO (.iso) paradrivers 
(set of drivers designed for virtual machines to improve I/O performance by leveraging paravirtualization techniques 
-  Paravirtualization involves modifying the guest operating system to work more efficiently with the virtualization platform,
making it aware that it is running in a virtualized environment. 
This awareness allows the guest OS to communicate more efficiently with the virtualization platform.)
The download can be performed from https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/?C=M;O=D
Add new device to the VM:
Add hardware - Storage - Select or create custom storage - Device type: CDROM - Manage.. - Select the downloaded VirtIO.iso - This now is SATA CDROM2
Remove the Tablet device
Display : Select the Type "Spice Server" for better performance or VNC if you access remotely to the host to use the VMs, Listen Type: None, Enable OpenGL
Video: Select the "VirtIO" model , Disable 3D acceleration
TPM: Advanced options: select Model TIS and Version 2.0
Now you have SATA Disk1, SATA CDROM1, SATA CDROM2. For SATA Disk 1 (the Vm storage disk) in Advanced options set Cache mode to none and Discard Mode to unmap.
Boot Options: Put SATA CDROM1 (the disk with the operating system) before VirtIO Disk 1 (the storage disk) for first booting. Then after the OS is installed you'll be able to boot from SATA Disk 1 (you can again change the boot order putting SATA Disk 1 as the first boot option). Just notice that in Windows 11 a first step will create the required partitions and a second reboot will ask you to install Windows on the primary partition.
At install Win11 requires network connection. To Bypass it, SHIFT(+FN)+F10 and type OOBE\BYPASSNRO. Then another rebooot will be performed but with the option of skipping the first network connection.
To skip Microsoft Account access, log in with a@a.com with a random password.
VirtualMachine - Redirect USB Device allow connection of USB devices to the Guest OS

Networking configuration:
Default option for newly created VM is NAT. You can change the device model to virtio (after you installed the VirtIO driver inside the host machine)
for better performance.
All guest VM connected in the same NAT network can communicate (Windows VMs must have the Firewall disabled..)
You can also connect your VM to a bridge network:
set the NIC as follows:
Network source : Bridge device
Device name: virbr0
Device model: virtio

When you're inside the Windows VM, go to the explore resources and then to the VirtIO.iso disk.
You can install the VirtIO drivers via the .msi installer file.
Then, reboot the system, go to display options and now you can select a different screen resolution.

SNAPSHOTS:
Guest OS Snapshots can be created from virtual-manager, 
via the Manage VM Snapshots icon. OS USING UEFI AS FIRMWARE (LIKE WINDOWS) MUST NOT BE RUNNING.
In the lower left you can add snapshots, and you can select them.
Then when you run the VM you'll run from that snapshot state.

GPU PASSTHROUGH --> https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF , https://ubuntu.com/server/docs/gpu-virtualization-with-qemu-kvm:
Current hardware configuration:
CPU with an integrated graphic card (to be assigned to the host) --> AMD Ryzen 5 5600G
A dedicated GPU (to be assigned to the guest OS) --> Nvidia GeForce GTX 1060 3GB
MOBO: ASUS PRIME B350M-K

First operations:
First I needed to update the motherboard UEFI BIOS from v6002 to 6042 (to make it compatible with the new CPU)
For ASUS mobos, just download from asus.com and then copy in a formatted USB pendrive the .CAP file for v6042 (new bios/firmware) in your pendrive.
Then, put it in the USB ports in the back of the PC, reboot the system and enter the UEFI BIOS. There, for this mobo
model at least, go to Advanced Options, Tools and search for the UEFI BIOS Update section, select your drive and .CAP file and confirm 
the UEFI BIOS update.
At the end of the process I was greeted with a missing keyboard error, but substituting my wireless keyboard with a 
wired one allowed me to procede to the new UEFI BIOS and then to the OS.
Remember to re-activate the CPU virtualization from the UEFI BIOS after the mobo firmware update (For AMD CPU, Enable SVM from Advanced options)
Repeating the network connection was required.
Then I replaced the CPU.
Required to re-activate the CPU virtualization from the UEFI BIOS after the CPU replacement (For AMD CPU, Enable SVM from Advanced options)
Repeating the network connection was also required.
In UEFI BIOS also enabled IOMMU (couldnt find its location though, I searched it with the search bar)
The system need to be able to use both the integrated and the dedicated GPU: in the UEFI BIOS (ASUS)
Advanced - NB Configuration - IGFX Multi-Monitor - Enable
BE CAREFUL THAT DOING THIS I WASNT ABLE ANYMORE TO ACCESS THE BIOS: I COULD ONLY SEE THE OS ONCE BOOTED.
I WAS CONNECTED WITH A SINGLE MONITOR TO THE DEDICATED GPU OUTPUT, SO APPARENTELY WITH THIS NEW SETUP 
THE INTEGRATED GRAPHICS WAS DETECTED BUT COULDN'T BE USED SINCE THE SWITCH TO DEDICATED GPU WAS PERFORMED 
AFTER BOOT. SO EVERYTHING BEFORE THE BOOTING AND AFTER THE SHUTDOWN WAS LOST. TO RECOVER THIS I CONNECTED ONE
MONITOR TO THE MOTHERBOARD AND ONE TO THE NVIDIA GPU, SO NOW THE INTEGRATED GRAPHICS HANDLES ONE MONITOR AND THE
DEDICATED ONE IS USED ON THE SECOND MONITOR. SO NOW BIOS CAN BE ACCESSED AGAIN, AT LEAST ON THE MOBO MONITOR.
In the host os now running lspci you should be able to see 2 VGA controllers, one being the integrated and one the dedicated GPU.
Executing check_iommu.sh allows you to see the PCi devices IOMMU groups (the smallest set of physical devices that can be passed
to a virtual machine) > in my case, running it I obtain:
IOMMU Group 11 09:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Cezanne [1002:1638] (rev c9)
IOMMU Group 9 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] [10de:1c02] (rev a1)
while running 
lspci | grep VGA
returns
01:00.0 VGA compatible controller: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] (rev a1)
09:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Cezanne (rev c9)
Which means PCi device with address 01:00.0 (GeForce GTX 1060 3GB) belongs to IOMMU group 11
while PCi device with address 09:00.0 ([AMD/ATI] Cezanne) belongs to IOMMU group 9
so they are in different groups and one of them can be dedicated to a VM without affecting the other.
Command 
lspci -k | grep -A 2 VGA  
also gives you a list of the drivers used by the GPUs.
you can also run
lspci -nnk
A complete driver list can be obtained running (for nvidia gpus) running
lsmod | grep -i nvidia
Which in my case returns
nvidia_uvm           1781760  0
nvidia_drm             94208  3
nvidia_modeset       1314816  3 nvidia_drm
nvidia              56745984  105 nvidia_uvm,nvidia_modeset
drm_kms_helper        249856  4 drm_display_helper,amdgpu,nvidia_drm
drm                   700416  23 gpu_sched,drm_kms_helper,drm_display_helper,nvidia,drm_buddy,amdgpu,drm_ttm_helper,nvidia_drm,ttm
video                  73728  3 asus_wmi,amdgpu,nvidia_modeset
Which means the drivers / kernel modules that handle the Nvidia GPUs are 
nvidia_drm, nvidia_modeset, nvidia_uvm and nvidia.
Enable IOMMU and update GRUB:
IOMMU must be enabled on your system. To do this, you have to modify the 
/etc/default/grub file with
sudo nano /etc/default/grub
and add or modify the line with GRUB_CMDLINE_LINUX 
to include amd_iommu=on or intel_iommu=on, depending on your processor. 
Then 
sudo update-grub 
to effectively update GRUB bootloader configuration file
(This file is typically located at /boot/grub/grub.cfg).
----
EXPLANATION: When the BIOS UEFI mobo firmware is executed, it searches for 
booitable devices which must have a bootloader program (in a dedicated boot sector,
which may be part of a boot partition of an hard disk) which is in charge of loading
the operating system kernel (which is included in the OS image that must be as well
inside a device to be defined as "bootable"..).
So the bootloader is a program that searches and loads in memory another program (the kernel),
so it can be executed. A typical bootloader for Linux distros is GRUB.
You can specify some command-line options that will be passed
to the Linux kernel when a system boots using the GRUB bootloader.
The GRUB_CMDLINE_LINUX variable allows you to configure various parameters related to the kernel
and the initial ramdisk (initrd).
IOMMU is an hardware component (phisycally present on the CPU, 
but the motherboard must have a chipset that supports it)
that provides memory and IO (Input/Output) address translation.
Its primary function is to map virtual addresses used by devices in a system
to physical addresses, allowing for efficient and secure communication
between the CPU, memory, and I/O devices.
IOMMU (Input-Output Memory Management Unit) is required for GPU passthrough in virtualization
to provide the ability for a virtual machine (VM) to directly access and control a physical GPU. 
Without IOMMU support, the hypervisor (virtualization software) would not be able 
to efficiently assign a dedicated GPU to a virtual machine due to certain limitations:
-there would be no hardware support for memory address translation and mapping,
which is essential for isolating the memory space of the GPU assigned to the VM:
The VM and the host system would share the same physical memory address space, making it difficult 
to isolate and protect the memory regions used by the GPU. 
This lack of isolation can lead to security vulnerabilities and potential data corruption.
-GPUs use DMA to transfer data directly to and from system memory.
Without IOMMU, there would be no efficient way to control and isolate these DMA transactions.
DMA transactions initiated by the GPU would not be properly translated and isolated, 
potentially leading to data leakage, unauthorized access to system memory, or corruption of data.
Without IOMMU support, the hypervisor would have difficulty in assigning a dedicated GPU directly to a VM.
Device assignment would be limited to emulated or virtualized GPU solutions,
which are typically slower and have reduced capabilities compared to direct GPU passthrough.
---
now run 
dmesg | grep -i -e DMAR -e IOMMU
And you should see some stuff (having no output means your hardware desnt support iommu.. and thats bad)

ISOLATING A GPU
In order to assign a device to a virtual machine, this device and all those sharing the same IOMMU group 
must have their driver replaced by a stub driver or a VFIO driver in order to prevent the host machine from interacting with them.
However, you cannot perform this substitution while the host system is using the GPU:
due to their size and complexity, GPU drivers do not tend to support dynamic rebinding very well, 
so you cannot simply have some GPU you use on the host be transparently passed to a virtual machine
without having both drivers conflict with each other.
Because of this, it is generally advised to bind those placeholder drivers manually
before starting the virtual machine, in order to stop other drivers from attempting to claim it.
So now we have to configure a GPU so those placeholder drivers are bound early during the boot process,
which makes said device inactive until a virtual machine claims it or the driver is switched back.
In my case, since 
lspci -nnk
returns
01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] [10de:1c02] (rev a1)
	Subsystem: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] [10de:1c02]
	Kernel driver in use: nvidia
	Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia
09:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Cezanne [1002:1638] (rev c9)
	Subsystem: ASUSTeK Computer Inc. Cezanne [1043:8809]
	Kernel driver in use: amdgpu
	Kernel modules: amdgpu
It means that NVIDIA GPU is using "nvidia" driver.
We have to replace this with a vfio-pci driver for GPU passthrough to work.
Vfio-pci normally targets PCI devices by ID, meaning you only need to specify the IDs of the devices you intend to passthrough.
So you find the devices by IOMMU ID and assign them a vfio driver to make the device effectively unaccessible by the host.
First of all, run the script to get the IOMMU groups and find your GPU devices:
IOMMU Group 11 09:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Cezanne [1002:1638] (rev c9)
IOMMU Group 9 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] [10de:1c02] (rev a1)
IOMMU Group 9 01:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
To pass your device id, you have 2 options:
1 - Modify as done above the GRUB_CMDLINE_LINUX in the file /etc/default/grub
sudo nano /etc/default/grub
by appending the following (in my case), to isolate both the GPU and its audio controller
vfio-pci.ids=10de:1c02,10de:10f1
then
sudo update-grub
2 - the IDs may be added to a modprobe conf file (create new or modify existing one)
sudo nano /etc/modprobe.d/vfio.conf
add a line:
options vfio-pci ids=10de:1c02,10de:10f1
EXPLANATION: /etc/modprobe.d is a Linux directory that contains
configuration files to control the automatic loading and unloading of kernel modules using the modprobe command
Vfio framework is composed by kernel modules, 
so you have to load them dynamically in the running Linux kernel to assign vfio drivers to your hardware devices.
To load the kerlem modules taking into account the new modifications of the modprobe configuration files:
sudo modprobe vfio
sudo modprobe vfio_pci
In this way, the configuration in vfio.conf is applied during the system's initialization process,
specifically when kernel modules are loaded.
then you also need to update the initramfs image
sudo update-initramfs -u
Initramfs (initial RAM filesystem) is a temporary filesystem that is loaded into memory
(by the grub bootloader) during the initial boot process before the root filesystem is mounted.
It is used to perform essential tasks required to initialize the system
and prepare it for the transition to the actual root filesystem.
The kernel then executes the initramfs to perform the necessary tasks before transitioning
to the root filesystem, allowing the full operating system to start.
In particular, it is used to load necessary kernel modules 
or drivers required for the operation of critical hardware components.
After you chose the approach you prefer (I chose nÂ° 1), reboot the system.
After you asked for loading the vfio-pci kernel module at kernel reboot,
you should perform an additional step to force this module to load 
before the graphics drivers have a chance to bind to the card:
in file 
sudo nano /etc/modprobe.d/vfio.conf
add a line:
softdep drm pre: vfio-pci
then again
sudo modprobe vfio
sudo modprobe vfio_pci
sudo update-initramfs -u
ALTERNATIVE METHODS NOT FOR UBUNTU
You have 3 choices:
1 - in /etc/mkinitcpio.conf, add MODULES=(... vfio_pci vfio vfio_iommu_type1 ...) and HOOKS=(... modconf ...)
2 - in /etc/booster.yaml , add modules_force_load: vfio_pci,vfio,vfio_iommu_type1
3 - in /etc/dracut.conf.d/10-vfio.conf, add force_drivers+=" vfio_pci vfio vfio_iommu_type1 "
mkinitcpio, booster and dracut are tools or systems used in different Linux distributions
to create and manage the initial ramdisk (initramfs)
then reboot
ON UBUNTU 22.04, THE METHOD THAT WORKED WAS TO USE MODPROBE WITH FILE VFIO.CONF
Now after reboot only the display connected to the integrated graphics (so to the video output
ports of the mobo) works, in fact running
lspci -nnk
returns, for the NVIDIA GPU and audio device:
01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] [10de:1c02] (rev a1)
	Subsystem: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] [10de:1c02]
	Kernel driver in use: vfio-pci
	Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia
01:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
	Subsystem: NVIDIA Corporation GP106 High Definition Audio Controller [10de:1c02]
	Kernel driver in use: vfio-pci
	Kernel modules: snd_hda_intel
So they are effectively isolated since it is using the vfio-pci driver.

CONFIGURE VM TO ALLOW GPU PASSTHROUGH
Since my output for command 
lspci | grep VGA
was
01:00.0 VGA compatible controller: NVIDIA Corporation GP106 [GeForce GTX 1060 3GB] (rev a1)
I edited the Windows11 .xml file adding, inside <devices> section, the following subsections:
<hostdev mode='subsystem' type='pci' managed='yes'>
  <driver name='vfio'/>
  <source>
    <address domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
  </source>
  <address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
</hostdev>

My virbr0 network bridge is also a pci device with the same ID of my graphic card:
So I changed the bus='0x01' for virbr0 to bus='0x11' (i still have network connection on the guest)
On the guest Windows 11 I uninstalled the NVIDIA graphic drivers and reinstalled the latest ones (546.33)
and I was finally able to get a graphic output to display 2, and the Nvidia GTX 1060 is correctly 
detected by the guest system.